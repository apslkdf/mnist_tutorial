{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "180.4%c:\\users\\aaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "def squash(x):\n",
    "    lengths2 = x.pow(2).sum(dim=2)\n",
    "    lengths = lengths2.sqrt()\n",
    "    x = x * (lengths2 / (1 + lengths2) / lengths).view(x.size(0), x.size(1), 1)\n",
    "    return x\n",
    "\n",
    "class AgreementRouting(nn.Module):\n",
    "    def __init__(self, input_caps, output_caps, n_iterations):\n",
    "        super(AgreementRouting, self).__init__()\n",
    "        self.n_iterations = n_iterations\n",
    "        self.b = nn.Parameter(torch.zeros((input_caps, output_caps)))\n",
    "        \n",
    "    def forward(self, u_predict):\n",
    "        batch_size, input_caps, output_caps, output_dim = u_predict.size()\n",
    "\n",
    "        c = F.softmax(self.b)\n",
    "        s = (c.unsqueeze(2) * u_predict).sum(dim=1)\n",
    "        v = squash(s)\n",
    "\n",
    "        if self.n_iterations > 0:\n",
    "            b_batch = self.b.expand((batch_size, input_caps, output_caps))\n",
    "            for r in range(self.n_iterations):\n",
    "                v = v.unsqueeze(1)\n",
    "                b_batch = b_batch + (u_predict * v).sum(-1)\n",
    "\n",
    "                c = F.softmax(b_batch.view(-1, output_caps)).view(-1, input_caps, output_caps, 1)\n",
    "                s = (c * u_predict).sum(dim=1)\n",
    "                v = squash(s)\n",
    "\n",
    "        return v\n",
    "\n",
    "class CapsLayer(nn.Module):\n",
    "    def __init__(self, input_caps, input_dim, output_caps, output_dim, routing_module):\n",
    "        super(CapsLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.input_caps = input_caps\n",
    "        self.output_dim = output_dim\n",
    "        self.output_caps = output_caps\n",
    "        self.weights = nn.Parameter(torch.Tensor(input_caps, input_dim, output_caps * output_dim))\n",
    "        self.routing_module = routing_module\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.input_caps)\n",
    "        self.weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, caps_output):\n",
    "        caps_output = caps_output.unsqueeze(2)\n",
    "        u_predict = caps_output.matmul(self.weights)\n",
    "        u_predict = u_predict.view(u_predict.size(0), self.input_caps, self.output_caps, self.output_dim)\n",
    "        v = self.routing_module(u_predict)\n",
    "        return v\n",
    "\n",
    "class PrimaryCapsLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_caps, output_dim, kernel_size, stride):\n",
    "        super(PrimaryCapsLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_caps * output_dim, kernel_size=kernel_size, stride=stride)\n",
    "        self.input_channels = input_channels\n",
    "        self.output_caps = output_caps\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv(input)\n",
    "        N, C, H, W = out.size()\n",
    "        out = out.view(N, self.output_caps, self.output_dim, H, W)\n",
    "\n",
    "        # will output N x OUT_CAPS x OUT_DIM\n",
    "        out = out.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        out = out.view(out.size(0), -1, out.size(4))\n",
    "        out = squash(out)\n",
    "        return out\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, routing_iterations, n_classes=10):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.primaryCaps = PrimaryCapsLayer(256, 32, 8, kernel_size=9, stride=2)  # outputs 6*6\n",
    "        self.num_primaryCaps = 32 * 6 * 6\n",
    "        routing_module = AgreementRouting(self.num_primaryCaps, n_classes, routing_iterations)\n",
    "        self.digitCaps = CapsLayer(self.num_primaryCaps, 8, n_classes, 16, routing_module)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.primaryCaps(x)\n",
    "        x = self.digitCaps(x)\n",
    "        probs = x.pow(2).sum(dim=2).sqrt()\n",
    "        return x, probs\n",
    "\n",
    "class ReconstructionNet(nn.Module):\n",
    "    def __init__(self, n_dim=16, n_classes=10):\n",
    "        super(ReconstructionNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_dim * n_classes, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 784)\n",
    "        self.n_dim = n_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        mask = Variable(torch.zeros((x.size()[0], self.n_classes)), requires_grad=False)\n",
    "        if next(self.parameters()).is_cuda:\n",
    "            mask = mask.cuda()\n",
    "        mask.scatter_(1, target.view(-1, 1), 1.)\n",
    "        mask = mask.unsqueeze(2)\n",
    "        x = x * mask\n",
    "        x = x.view(-1, self.n_dim * self.n_classes)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class CapsNetWithReconstruction(nn.Module):\n",
    "    def __init__(self, capsnet, reconstruction_net):\n",
    "        super(CapsNetWithReconstruction, self).__init__()\n",
    "        self.capsnet = capsnet\n",
    "        self.reconstruction_net = reconstruction_net\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x, probs = self.capsnet(x)\n",
    "        reconstruction = self.reconstruction_net(x, target)\n",
    "        return reconstruction, probs\n",
    "\n",
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self, m_pos, m_neg, lambda_):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.m_pos = m_pos\n",
    "        self.m_neg = m_neg\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, lengths, targets, size_average=True):\n",
    "        t = torch.zeros(lengths.size()).long()\n",
    "        if targets.is_cuda:\n",
    "            t = t.cuda()\n",
    "        t = t.scatter_(1, targets.data.view(-1, 1), 1)\n",
    "        targets = Variable(t)\n",
    "        losses = targets.float() * F.relu(self.m_pos - lengths).pow(2) + \\\n",
    "                 self.lambda_ * (1. - targets.float()) * F.relu(lengths - self.m_neg).pow(2)\n",
    "        return losses.mean() if size_average else losses.sum()\n",
    "   \n",
    "    \n",
    "model = CapsNet(routing_iterations=3)\n",
    "\n",
    "\n",
    "# TODO:define loss function and optimiter\n",
    "criterion = MarginLoss(0.9, 0.1, 0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=15, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\aaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\users\\aaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\users\\aaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.080333\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.056642\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.035982\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.021942\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.017979\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.014543\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.012426\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.009879\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.011318\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.009195\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.005931\n",
      "Train Epoch: 1 [14080/60000 (24%)]\tLoss: 0.007081\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.005147\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.004764\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.005303\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.004805\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.004952\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.005790\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.005761\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.003590\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.004201\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.003295\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.004049\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.002847\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.003912\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.003417\n",
      "Train Epoch: 1 [33280/60000 (56%)]\tLoss: 0.003440\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.002460\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.002449\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.002937\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.004757\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.003924\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.003142\n",
      "Train Epoch: 1 [42240/60000 (71%)]\tLoss: 0.003007\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 0.001513\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.002895\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.001824\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.002291\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.003078\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.001566\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.003697\n",
      "Train Epoch: 1 [52480/60000 (88%)]\tLoss: 0.002459\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.002461\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.001848\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.002972\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.002010\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.002491\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 128/10000 (1%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.002827\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.002238\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.002275\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.001801\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.001754\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.001721\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.001619\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.002672\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.001611\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.002035\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.001433\n",
      "Train Epoch: 2 [14080/60000 (24%)]\tLoss: 0.003255\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.001415\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.001090\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.002282\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.001824\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.001535\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.002192\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.002507\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.001605\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.001196\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.001200\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.000941\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.001099\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.001168\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.001139\n",
      "Train Epoch: 2 [33280/60000 (56%)]\tLoss: 0.001568\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.001559\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.000779\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.001516\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.001200\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.001693\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.003057\n",
      "Train Epoch: 2 [42240/60000 (71%)]\tLoss: 0.002722\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 0.000946\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.001196\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.001992\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.001094\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.001525\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.001369\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.001704\n",
      "Train Epoch: 2 [52480/60000 (88%)]\tLoss: 0.000860\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.001202\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.000599\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.002377\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.001088\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.001124\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 127/10000 (1%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.000909\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.001009\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.000669\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.000991\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.001052\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001165\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.000776\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.001277\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.000743\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.001970\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.001732\n",
      "Train Epoch: 3 [14080/60000 (24%)]\tLoss: 0.000718\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.001921\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.000679\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.001340\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.001064\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.001456\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.000859\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.000432\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.001199\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.001511\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.001098\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.000454\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.001316\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.001130\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001187\n",
      "Train Epoch: 3 [33280/60000 (56%)]\tLoss: 0.001692\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.000890\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.001394\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.000832\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.001422\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.000727\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.002258\n",
      "Train Epoch: 3 [42240/60000 (71%)]\tLoss: 0.000924\n",
      "Train Epoch: 3 [43520/60000 (73%)]\tLoss: 0.001193\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.000929\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.001440\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.001912\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.001662\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.000999\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.000974\n",
      "Train Epoch: 3 [52480/60000 (88%)]\tLoss: 0.000481\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.000489\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.000954\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.000874\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.000977\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.001088\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 128/10000 (1%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.000749\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.000912\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.000848\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.000899\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.000677\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.000432\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.000736\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.000543\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.000569\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.001033\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.000925\n",
      "Train Epoch: 4 [14080/60000 (24%)]\tLoss: 0.001616\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.000649\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.000474\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.000647\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.000740\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.000576\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.001059\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.000293\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.000701\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.000976\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.000772\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.000569\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.000593\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.001225\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.000646\n",
      "Train Epoch: 4 [33280/60000 (56%)]\tLoss: 0.000913\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.001111\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.001815\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.000861\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001633\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.000895\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.001686\n",
      "Train Epoch: 4 [42240/60000 (71%)]\tLoss: 0.001210\n",
      "Train Epoch: 4 [43520/60000 (73%)]\tLoss: 0.001149\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.000633\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.001430\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.000636\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.000905\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.000913\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001280\n",
      "Train Epoch: 4 [52480/60000 (88%)]\tLoss: 0.000794\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.000655\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.001625\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.003337\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.000950\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.001089\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 127/10000 (1%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000330\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.000733\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.000823\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.000400\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.000543\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.000252\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.000611\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.000309\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.000861\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.000422\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.000213\n",
      "Train Epoch: 5 [14080/60000 (24%)]\tLoss: 0.000429\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.000865\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.000291\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.000287\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.000285\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.000425\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.000878\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.000619\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.000505\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.000468\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.000451\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.000334\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.000258\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.000695\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.000679\n",
      "Train Epoch: 5 [33280/60000 (56%)]\tLoss: 0.000389\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.000816\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.000525\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.000934\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.001098\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e867dfdc525e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# TODO:forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-e867dfdc525e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "# train and evaluate\n",
    "log_interval = 10\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target, requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        output, probs = model(data)\n",
    "        loss = criterion(probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        output, probs = model(data)\n",
    "        test_loss += criterion(probs, target, size_average=False).item()\n",
    "        pred = probs.data.max(1, keepdim=True)[1]  # get the index of the max probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        return test_loss\n",
    "\n",
    "for epoch in range(1,NUM_EPOCHS+1):\n",
    "    # TODO:forward + backward + optimize\n",
    "    train(epoch)\n",
    "    test_loss = test()\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    # evaluate\n",
    "    # TODO:calculate the accuracy using traning and testing dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\aaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\users\\aaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Training accuracy: 99.71%\n",
      "Testing accuracy: 99.11%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for images,labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs[1], 1)\n",
    "    total += labels.size(0)\n",
    "    correct += np.array(predicted == labels).astype(int).sum().item()\n",
    "print('Training accuracy: %.2f%%' % (100*correct/total))\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for images,labels in test_loader:\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs[1], 1)\n",
    "    total += labels.size(0)\n",
    "    correct += np.array(predicted == labels).astype(int).sum().item()\n",
    "print('Testing accuracy: %.2f%%' % (100*correct/total))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}